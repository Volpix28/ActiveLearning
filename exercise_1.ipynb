{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score, pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl geladener Bilder: 25000\n",
      "Form eines Bildes: (64, 64)\n",
      "Erste 5 Labels: [1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_image(image_path, size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Lädt ein Bild von einem gegebenen Pfad, ändert dessen Größe und konvertiert es in Graustufen.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize(size).convert('L')\n",
    "    image_array = np.array(image) / 255.0\n",
    "    \n",
    "    return image_array\n",
    "\n",
    "def load_and_preprocess_images(directory, csv_path, size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Lädt und verarbeitet alle Bilder in einem Verzeichnis basierend auf einer CSV-Datei, die Labels enthält.\n",
    "    \"\"\"\n",
    "    # CSV-Datei laden, die Labels und Bildpfade enthält\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "\n",
    "            image_array = load_and_preprocess_image(file_path, size)\n",
    "            images.append(image_array)\n",
    "            # Das Label aus der CSV-Datei extrahieren\n",
    "            label = labels_df[labels_df['image'] == filename]['labels'].values[0]\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "directory = 'dataset/cat_dog'\n",
    "csv_path = 'dataset/cat_dog.csv'\n",
    "\n",
    "images, labels = load_and_preprocess_images(directory, csv_path)\n",
    "\n",
    "print(f\"Anzahl geladener Bilder: {len(images)}\")\n",
    "print(f\"Form eines Bildes: {images[0].shape}\")\n",
    "print(f\"Erste 5 Labels: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition (simple cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  # 16 Filter, Kernel-Größe 3\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # 32 Filter, Kernel-Größe 3\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)      # Vollverbundene Schicht\n",
    "        self.fc2 = nn.Linear(128, 2)                 # Ausgangsschicht\n",
    "        self.pool = nn.MaxPool2d(2, 2)               # Pooling-Schicht\n",
    "        self.relu = nn.ReLU()                        # ReLU Aktivierungsfunktion\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufteilung der Daten und Umwandlung in Tensoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Trainingsdaten: 5625\n",
      "Anzahl Testdaten: 625\n",
      "Anzahl 'ungelabelter' Daten: 18750\n"
     ]
    }
   ],
   "source": [
    "tensor_images = torch.tensor(images).float()\n",
    "tensor_labels = torch.tensor(labels).long()\n",
    "\n",
    "total_count = len(tensor_labels)\n",
    "\n",
    "num_labeled = int(0.25 * total_count)\n",
    "num_test = int(0.1 * num_labeled)\n",
    "\n",
    "# Indizes mischen und aufteilen\n",
    "indices = torch.randperm(total_count)\n",
    "labeled_indices = indices[:num_labeled]\n",
    "test_indices = labeled_indices[:num_test]  # Testindizes von den gelabelten Indizes nehmen\n",
    "train_indices = labeled_indices[num_test:]  # Restliche gelabelte Daten für Training\n",
    "unlabeled_indices = indices[num_labeled:]\n",
    "\n",
    "# Erstellen von Datenladern\n",
    "test_dataset = TensorDataset(tensor_images[test_indices], tensor_labels[test_indices])\n",
    "train_dataset = TensorDataset(tensor_images[train_indices], tensor_labels[train_indices])\n",
    "# unlabeled_dataset = TensorDataset(tensor_images[unlabeled_indices], torch.zeros(len(unlabeled_indices))) # Keine Labels für ungelabelte Daten -> für manuelles Labeln\n",
    "unlabeled_dataset = TensorDataset(tensor_images[unlabeled_indices], tensor_labels[unlabeled_indices]) # Für simulation des human-in-the-loop\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Anzahl Trainingsdaten: {len(train_loader.dataset)}\")\n",
    "print(f\"Anzahl Testdaten: {len(test_loader.dataset)}\")\n",
    "print(f\"Anzahl 'ungelabelter' Daten: {len(unlabeled_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiales Training als Vergleichsbasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 88/88 [00:06<00:00, 14.08it/s, loss=0.691]\n",
      "Epoch 2: 100%|██████████| 88/88 [00:06<00:00, 14.28it/s, loss=0.668]\n",
      "Epoch 3: 100%|██████████| 88/88 [00:06<00:00, 13.54it/s, loss=0.586]\n",
      "Epoch 4: 100%|██████████| 88/88 [00:06<00:00, 13.64it/s, loss=0.609]\n",
      "Epoch 5: 100%|██████████| 88/88 [00:07<00:00, 12.13it/s, loss=0.59] \n",
      "Epoch 6: 100%|██████████| 88/88 [00:07<00:00, 11.73it/s, loss=0.562]\n",
      "Epoch 7: 100%|██████████| 88/88 [00:06<00:00, 12.86it/s, loss=0.556]\n",
      "Epoch 8: 100%|██████████| 88/88 [00:06<00:00, 13.05it/s, loss=0.632]\n",
      "Epoch 9: 100%|██████████| 88/88 [00:06<00:00, 13.67it/s, loss=0.566]\n",
      "Epoch 10: 100%|██████████| 88/88 [00:06<00:00, 13.80it/s, loss=0.406]\n",
      "Epoch 11: 100%|██████████| 88/88 [00:06<00:00, 13.44it/s, loss=0.301]\n",
      "Epoch 12: 100%|██████████| 88/88 [00:06<00:00, 13.05it/s, loss=0.405]\n",
      "Epoch 13: 100%|██████████| 88/88 [00:06<00:00, 13.39it/s, loss=0.29] \n",
      "Epoch 14: 100%|██████████| 88/88 [00:06<00:00, 13.77it/s, loss=0.146]\n",
      "Epoch 15: 100%|██████████| 88/88 [00:06<00:00, 13.19it/s, loss=0.139] \n",
      "Epoch 16: 100%|██████████| 88/88 [00:06<00:00, 13.24it/s, loss=0.134] \n",
      "Epoch 17: 100%|██████████| 88/88 [00:07<00:00, 12.03it/s, loss=0.0678]\n",
      "Epoch 18: 100%|██████████| 88/88 [00:07<00:00, 12.47it/s, loss=0.0507]\n",
      "Epoch 19: 100%|██████████| 88/88 [00:06<00:00, 12.86it/s, loss=0.02]  \n",
      "Epoch 20: 100%|██████████| 88/88 [00:06<00:00, 13.49it/s, loss=0.0321] \n"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for images, labels in progress_bar:\n",
    "            # Format (batch_size, channels, width, height)\n",
    "            images = images.unsqueeze(1)  # Dimension für den Kanal\n",
    "            \n",
    "            # Feed Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backprop.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, epochs=20)\n",
    "torch.save(model.state_dict(), 'init_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genauigkeit auf Testdaten: 0.6928\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.unsqueeze(1)  # Dimension für den Kanal hinzufügen\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.numpy())\n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Genauigkeit auf Testdaten: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Sampling\n",
    "\n",
    "gewählte Methoden \n",
    "\n",
    "1. Entropie \n",
    "2. Lowest Confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:07<00:00, 12.54it/s, loss=0.601]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:07<00:00, 12.44it/s, loss=0.681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Genauigkeit auf Testdaten: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 12.77it/s, loss=0.645]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.86it/s, loss=0.616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Genauigkeit auf Testdaten: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.82it/s, loss=0.47] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.93it/s, loss=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.28it/s, loss=0.465]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.52it/s, loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.09it/s, loss=0.403]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.05it/s, loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.35it/s, loss=0.465]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.01it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Genauigkeit auf Testdaten: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.67it/s, loss=0.25] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.03it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.05it/s, loss=0.316]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.84it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, Genauigkeit auf Testdaten: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.35it/s, loss=0.161]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.54it/s, loss=0.206] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.53it/s, loss=0.123] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.00it/s, loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.05it/s, loss=0.693]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.71it/s, loss=0.637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Genauigkeit auf Testdaten: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.64it/s, loss=0.579]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.37it/s, loss=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Genauigkeit auf Testdaten: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.06it/s, loss=0.569]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.69it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.63it/s, loss=0.41] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.91it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Genauigkeit auf Testdaten: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 12.85it/s, loss=0.321]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.16it/s, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Genauigkeit auf Testdaten: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.25it/s, loss=0.213]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.97it/s, loss=0.0678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.07it/s, loss=0.153] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.14it/s, loss=0.0685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.11it/s, loss=0.0473]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.13it/s, loss=0.021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, Genauigkeit auf Testdaten: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.06it/s, loss=0.0767] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.07it/s, loss=0.0147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Genauigkeit auf Testdaten: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.02it/s, loss=0.00974]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.98it/s, loss=0.0106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Genauigkeit auf Testdaten: 0.71\n",
      "Genauigkeit mit Entropie-Unsicherheit: 0.72\n",
      "Genauigkeit mit Konfidenz-Unsicherheit: 0.71\n"
     ]
    }
   ],
   "source": [
    "def predict_uncertainty(model, data_loader):\n",
    "    model.eval()\n",
    "    uncertainties = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.unsqueeze(1)  # Dimension für den Kanal hinzufügen\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            entropy = -torch.sum(probabilities * torch.log(probabilities), dim=1)\n",
    "            uncertainties.extend(entropy.tolist())\n",
    "    return uncertainties\n",
    "\n",
    "def predict_confidence(model, data_loader):\n",
    "    model.eval()\n",
    "    confidences = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.unsqueeze(1)  # Dimension für den Kanal hinzufügen\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            max_confidence, _ = torch.max(probabilities, dim=1)\n",
    "            confidences.extend((1 - max_confidence).tolist())\n",
    "    return confidences\n",
    "\n",
    "def active_learning_cycle(unlabeled_loader_setting, train_loader, test_loader, criterion, optimizer, num_iterations=10, num_to_label=50, uncertainty_method=\"entropy\"):\n",
    "\n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        # Berechne Unsicherheiten\n",
    "        if uncertainty_method == \"entropy\":\n",
    "            uncertainties = predict_uncertainty(model, unlabeled_loader_setting)\n",
    "        elif uncertainty_method == \"confidence\":\n",
    "            uncertainties = predict_confidence(model, unlabeled_loader_setting)\n",
    "        else:\n",
    "            raise ValueError(\"Ungültige Uncertainty-Methode\")\n",
    "        \n",
    "        uncertain_indices = sorted(range(len(uncertainties)), key=lambda i: uncertainties[i], reverse=True)[:num_to_label]\n",
    "        \n",
    "        # Trainingsdaten mit gesampelten Daten kombinieren\n",
    "        new_images = torch.stack([unlabeled_loader_setting.dataset[i][0] for i in uncertain_indices])\n",
    "        new_labels = torch.tensor([unlabeled_loader_setting.dataset[i][1] for i in uncertain_indices])\n",
    "        combined_images = torch.cat((train_loader.dataset.tensors[0], new_images), dim=0)\n",
    "        combined_labels = torch.cat((train_loader.dataset.tensors[1], new_labels), dim=0)\n",
    "        combined_dataset = TensorDataset(combined_images, combined_labels)\n",
    "        combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Training\n",
    "        train(model, combined_loader, criterion, optimizer, epochs=2)\n",
    "        \n",
    "        # Eval.\n",
    "        test_accuracy = evaluate(model, test_loader)\n",
    "        print(f\"Iteration {iteration + 1}, Genauigkeit auf Testdaten: {test_accuracy:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncertainty Sampling mit Entropie\n",
    "model_entropy = active_learning_cycle(unlabeled_loader, train_loader, test_loader, criterion, optimizer, uncertainty_method=\"entropy\")\n",
    "torch.save(model_entropy.state_dict(), 'uncertain_entropy_model.pth')\n",
    "\n",
    "# Uncertainty Sampling mit Konfidenzwerten\n",
    "model_confidence = active_learning_cycle(unlabeled_loader, train_loader, test_loader, criterion, optimizer, uncertainty_method=\"confidence\")\n",
    "torch.save(model_confidence.state_dict(), 'uncertain_confidence_model.pth')\n",
    "\n",
    "# Vergleich\n",
    "accuracy_entropy = evaluate(model_entropy, test_loader)\n",
    "accuracy_confidence = evaluate(model_confidence, test_loader)\n",
    "\n",
    "print(f\"Genauigkeit mit Entropie-Unsicherheit: {accuracy_entropy:.2f}\")\n",
    "print(f\"Genauigkeit mit Konfidenz-Unsicherheit: {accuracy_confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diversity sampling\n",
    "\n",
    "Methoden implementiert:\n",
    "\n",
    "1. Cluster-based Sampling\n",
    "\n",
    "2. Farthest-first Traversal (größter Abstand zwischen Merkmalen, berechnet mithilfe von `pairwise_distances`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.55it/s, loss=0.692]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.16it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Genauigkeit auf Testdaten: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.86it/s, loss=0.543]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.23it/s, loss=0.63] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Genauigkeit auf Testdaten: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.74it/s, loss=0.506]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.17it/s, loss=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Genauigkeit auf Testdaten: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:07<00:00, 12.55it/s, loss=0.558]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.25it/s, loss=0.537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Genauigkeit auf Testdaten: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.68it/s, loss=0.474]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.23it/s, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Genauigkeit auf Testdaten: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.84it/s, loss=0.351]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.13it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Genauigkeit auf Testdaten: 0.7344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.77it/s, loss=0.391]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.25it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Genauigkeit auf Testdaten: 0.6992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.79it/s, loss=0.414]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.25it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, Genauigkeit auf Testdaten: 0.6992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.00it/s, loss=0.242] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.19it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Genauigkeit auf Testdaten: 0.6976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.77it/s, loss=0.174] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.05it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Genauigkeit auf Testdaten: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.98it/s, loss=0.655]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.29it/s, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Genauigkeit auf Testdaten: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.63it/s, loss=0.512]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.41it/s, loss=0.584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Genauigkeit auf Testdaten: 0.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.50it/s, loss=0.549]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.50it/s, loss=0.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Genauigkeit auf Testdaten: 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.07it/s, loss=0.364]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.34it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Genauigkeit auf Testdaten: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.78it/s, loss=0.374]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.96it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Genauigkeit auf Testdaten: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:07<00:00, 12.47it/s, loss=0.302]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 13.84it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Genauigkeit auf Testdaten: 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.81it/s, loss=0.141] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.30it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Genauigkeit auf Testdaten: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.16it/s, loss=0.078] \n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.64it/s, loss=0.021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, Genauigkeit auf Testdaten: 0.7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 14.36it/s, loss=0.0339]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.82it/s, loss=0.0402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Genauigkeit auf Testdaten: 0.7088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 89/89 [00:06<00:00, 13.79it/s, loss=0.00911]\n",
      "Epoch 2: 100%|██████████| 89/89 [00:06<00:00, 14.32it/s, loss=0.0201] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Genauigkeit auf Testdaten: 0.7232\n",
      "Genauigkeit mit Cluster-based Sampling: 0.7024\n",
      "Genauigkeit mit Farthest-first Traversal: 0.7232\n"
     ]
    }
   ],
   "source": [
    "def diversity_sampling_cluster(model, unlabelled_loader, num_to_label):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in unlabelled_loader:\n",
    "            images = images.unsqueeze(1)  # Dimension für den Kanal hinzufügen\n",
    "            outputs = model(images)\n",
    "            features.extend(outputs.tolist())\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_to_label, random_state=0).fit(features)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    diverse_indices = []\n",
    "    for center in cluster_centers:\n",
    "        distances = pairwise_distances([center], features)[0]\n",
    "        closest_index = distances.argmin()\n",
    "        diverse_indices.append(closest_index)\n",
    "    \n",
    "    return diverse_indices\n",
    "\n",
    "def diversity_sampling_farthest(model, unlabelled_loader, num_to_label):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in unlabelled_loader:\n",
    "            images = images.unsqueeze(1)  # Dimension für den Kanal hinzufügen\n",
    "            outputs = model(images)\n",
    "            features.extend(outputs.tolist())\n",
    "    \n",
    "    diverse_indices = [0]  # Startpunkt f. berechnung der distanzen\n",
    "    for _ in range(num_to_label - 1):\n",
    "        distances = pairwise_distances([features[diverse_indices[-1]]], features)[0]\n",
    "        farthest_index = distances.argmax()\n",
    "        diverse_indices.append(farthest_index)\n",
    "    \n",
    "    return diverse_indices\n",
    "\n",
    "def active_learning_cycle(unlabeled_loader_setting, train_loader, test_loader, criterion, optimizer, num_iterations=10, num_to_label=50, diversity_method=\"cluster\"):\n",
    "\n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        # Berechne Diversität\n",
    "        if diversity_method == \"cluster\":\n",
    "            diverse_indices = diversity_sampling_cluster(model, unlabeled_loader_setting, num_to_label)\n",
    "        elif diversity_method == \"farthest\":\n",
    "            diverse_indices = diversity_sampling_farthest(model, unlabeled_loader_setting, num_to_label)\n",
    "        else:\n",
    "            raise ValueError(\"Ungültige Diversity-Methode\")\n",
    "        \n",
    "        # Trainingsdaten mit gesampelten Daten kombinieren\n",
    "        new_images = torch.stack([unlabeled_loader_setting.dataset[i][0] for i in diverse_indices])\n",
    "        new_labels = torch.tensor([unlabeled_loader_setting.dataset[i][1] for i in diverse_indices])\n",
    "        \n",
    "        combined_images = torch.cat((train_loader.dataset.tensors[0], new_images), dim=0)\n",
    "        combined_labels = torch.cat((train_loader.dataset.tensors[1], new_labels), dim=0)\n",
    "        combined_dataset = TensorDataset(combined_images, combined_labels)\n",
    "        combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Training\n",
    "        train(model, combined_loader, criterion, optimizer, epochs=2)\n",
    "        \n",
    "        # Eval.\n",
    "        test_accuracy = evaluate(model, test_loader)\n",
    "        print(f\"Iteration {iteration + 1}, Genauigkeit auf Testdaten: {test_accuracy}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Diversity Sampling mit Cluster-based Sampling\n",
    "model_cluster = active_learning_cycle(unlabeled_loader, train_loader, test_loader, criterion, optimizer, diversity_method=\"cluster\")\n",
    "torch.save(model_cluster.state_dict(), 'diversity_cluster_model.pth')\n",
    "\n",
    "# Diversity Sampling mit Farthest-first Traversal\n",
    "model_farthest = active_learning_cycle(unlabeled_loader, train_loader, test_loader, criterion, optimizer, diversity_method=\"farthest\")\n",
    "torch.save(model_farthest.state_dict(), 'diversity_farthest_model.pth')\n",
    "\n",
    "# Vergleich\n",
    "accuracy_cluster = evaluate(model_cluster, test_loader)\n",
    "accuracy_farthest = evaluate(model_farthest, test_loader)\n",
    "\n",
    "print(f\"Genauigkeit mit Cluster-based Sampling: {accuracy_cluster}\")\n",
    "print(f\"Genauigkeit mit Farthest-first Traversal: {accuracy_farthest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleiche die Genauigkeiten aller Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genauigkeit ohne Active Learning: 0.6928\n",
      "Genauigkeit mit Entropie-Unsicherheit: 0.7184\n",
      "Genauigkeit mit Konfidenz-Unsicherheit: 0.712\n",
      "Genauigkeit mit Cluster-based Sampling: 0.7024\n",
      "Genauigkeit mit Farthest-first Traversal: 0.7232\n"
     ]
    }
   ],
   "source": [
    "print(f\"Genauigkeit ohne Active Learning: {test_accuracy}\")\n",
    "print(f\"Genauigkeit mit Entropie-Unsicherheit: {accuracy_entropy}\")\n",
    "print(f\"Genauigkeit mit Konfidenz-Unsicherheit: {accuracy_confidence}\")\n",
    "print(f\"Genauigkeit mit Cluster-based Sampling: {accuracy_cluster}\")\n",
    "print(f\"Genauigkeit mit Farthest-first Traversal: {accuracy_farthest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusio\n",
    "\n",
    "Beide Ansätze, Uncertainty Sampling und Diversity Sampling, haben die Genauigkeit im Vergleich zum Modell ohne Active Learning verbessert. Das ist schon mal ein gutes Zeichen und zeigt, dass das samplen der Trainingsdaten während des Trainings zumindest irgendwas bewirkt hat. Bei den Uncertainty-Sampling-Methoden hat die Entropie-basierte Methode eine etwas höhere Genauigkeit erreicht als die Konfidenz-basierte Methode - ist aber in Anbetracht der wenigen Epochen und des generell sehr minimal gehaltenen Experimentes nicht unbedingt aussagekräftig. Das könnte aber bedeuten, dass es in diesem Fall besser war, die Unsicherheit über alle Vorhersagen (Entropie) zu berücksichtigen, anstatt nur die Konfidenz der am wahrscheinlichsten vorhergesagten Klasse zu verwenden. Wenn wir uns die Diversity-Sampling-Methoden anschauen, sehen wir, dass die Farthest-first Traversal-Methode besser abgeschnitten hat als die Cluster-based Sampling-Methode. Das erklär ich mir so, dass es in diesem Fall vorteilhafter war, Beispiele auszuwählen, die sich stark von den bereits Ausgewählten unterscheiden, anstatt Clustering zur Identifizierung von Ausreißern zu verwenden.\n",
    "Insgesamt hat die Farthest-first Traversal-Methode von allen getesteten Methoden die höchste Genauigkeit erzielt. I guess, dass die Erkundung des \"Merkmalsraums\" durch die Auswahl stärker unterschiedlicher Beispiele bei dem Klassifizierungsproblem (Katze vs. Hund) effektiv war. Nur eine Vermutung, aber ich kann mir vorstellen das grad bei Bilders eine größere Vielfalt mehr Sinn macht als Ausreißer, da viel generalisiert werden muss (?).\n",
    "\n",
    "**Vor- und Nachteilen der beiden Ansätze:**\n",
    "\n",
    "\n",
    "*Uncertainty Sampling:*\n",
    "\n",
    "Vorteil: Es konzentriert sich auf Beispiele, bei denen das Modell unsicher ist, was dazu beitragen kann, die Entscheidungsgrenzen des Modells zu verfeinern.\n",
    "Nachteil: Es besteht die Gefahr, sich auf schwierige oder sogar fehlerhafte Beispiele zu konzentrieren, was zu Overfitting führen kann.\n",
    "\n",
    "*Diversity Sampling:*\n",
    "\n",
    "Vorteil: Es fördert die Auswahl vielfältiger Beispiele, was dazu beitragen kann, eine breitere Abdeckung des \"Merkmalsraums\" zu erreichen und die Generalisierungsfähigkeit des Modells zu verbessern.\n",
    "Nachteil: Die Leistung hängt von der Qualität der verwendeten Methode zur Messung der Vielfalt ab (z. B. Clustering oder Abstandsmetriken).\n",
    "\n",
    "Für mich hat sich herausgestellt, dass beim Active Learning, insbesondere mit Farthest-first Traversal, eine Verbesserung der Klassifizierungsleistung bei diesem Problem zu erreichen ist. Beide Varianten miteinander zu verbinden wäre natürlich auch noch interessant :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manuelle daten labelung \n",
    ".. aus dem ersten Versuch - hab dann bestehende labels genutzt weil Zeit :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageOps\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# import pandas as pd\n",
    "# from torchvision import transforms\n",
    "# from IPython.display import display, clear_output\n",
    "# \n",
    "# # Funktion zur manuellen Beschriftung der Bilder mit größerer Bildanzeige\n",
    "# def manual_labeling(data_loader, indices):\n",
    "#     # Leeres DataFrame für gelabelte Daten\n",
    "#     labeled_data = pd.DataFrame(columns=['label'])\n",
    "# \n",
    "#     for i, (images, _) in enumerate(data_loader):\n",
    "#         img = transforms.ToPILImage()(images.squeeze(0))\n",
    "#         img = img.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "#         display(img)  # Verwende display anstatt img.show()\n",
    "#         label = input(\"Label eingeben (0 für Katze, 1 für Hund): \")\n",
    "#         clear_output(wait=True)\n",
    "#         labeled_data.loc[i] = [int(label)]\n",
    "# \n",
    "#     return labeled_data\n",
    "# \n",
    "# # Bereite den DataLoader vor, der nur die unsicheren Daten enthält\n",
    "# uncertain_data_loader = Subset(ungelabeled_loader.dataset, uncertain_indices)\n",
    "# uncertain_loader = DataLoader(uncertain_data_loader, batch_size=1, shuffle=False)\n",
    "# \n",
    "# # Labeln der unsicheren Daten manuell\n",
    "# labeled_uncertain_data = manual_labeling(uncertain_loader, uncertain_indices)\n",
    "# labeled_uncertain_data.to_csv('labeled_uncertain_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
